# Bert Example
This example follows https://github.com/google-research/bert to show how to run bert retaining on UAI Platform using docker.

\*\*\*\*\* April 29th 2019: Chinese XNLI Example for UAI \*\*\*\*\*

We give the first example for running XNLI task on UAI with BERT-Base, Chinese model.

## Setup
### Download the BERT-Base, Chinese model or Use Prebuild BERT-Base-Chinese Docker
*  You can directly download the official BERT-Base, Chinese model: **[`BERT-Base, Chinese`](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)**:
    Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
    parameters
*  You can pull the prebuild docker with BERT-Base Chinese Model already packed from uhub.service.ucloud.cn/uaishare/bert:chinese\_l12\_h768\_a12. The docker is based on tensorflow-1.13.1-py3.5.

### Get Training/Eval Data of XNLI
It depends on two dataset from https://github.com/facebookresearch/XNLI. 
*  Training Dataset: [XNLI-MT 1.0](https://dl.fbaipublicfiles.com/XNLI/XNLI-MT-1.0.zip) (445MB, ZIP)
*  Eval Dataset: [XNLI 1.0](https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip) (17MB, ZIP)

## UAI Example
### Chinese XNLI
We do following modifications to the run\_classifier.py:
1. Add UAI SDK related arguments: --data\_dir, --output\_dir, --work\_dir, --log\_dir, --num\_gpus, these arguments are auto generated by UAI Train Platform, see: https://github.com/ucloud/uai-sdk/blob/master/uaitrain/arch/tensorflow/uflag.py for more details

#### Build Training Docker Image
We assume you fully understand how UAI Train docker image works and has already read the DOCS here: https://docs.ucloud.cn/ai/uai-train/guide/tensorflow.

We provide bert\_train.Dockerfile for you to build the training image yourself. Before build the image, you should arrange your working directory as follows:

```
/_ WORK_DIRECTORY/
  |_ bert_train.Dockerfile
  |_ chinese_L-12_H-768_A-12/
    |_ bert_config.json
    |_ bert_model.ckpt.data-00000-of-00001
    |_ ...
    |_ vocab.txt
  |_ bert/
    |_ __init__.py
    |_ create_pretraining_data.py
    |_ ...
    |_ run_classifier.py
    |_ ...
```

Take the following CMD to build your own image:

```
sudo docker build -t uhub.service.ucloud.cn/YOUR_UHUB_REGISTRY/bert:chinese_base -f bert_train.Dockerfile .
```

You can push this image into your own UCLOUD hub.

#### Running the Training locally
You should parepare your XNLI data with following CMDS:

```
cp -r /PATH_TO_XNLI-MT-1.0/multinli/ /data/bert/
cp /PATH_TO_XNLI-1.0/* /data/bert/
```

Then the data will be organized as follows:

```
/data/bert/
  |_ XNLI/
    |_ multinli/
      |_ multinli.train.ar.tsv
      |_ ...
      |_ multinli.train.zh.tsv
    |_ xnli.dev.jsonl
    |_ xnli.dev.tsv
    |_ xnli.test.jsonl
    |_ xnli.test.tsv
```

You can run the training locally as
```
sudo nvidia-docker run -v /data/bert/XNLI/:/data/data/ -v /tmp/output/:/data/output uhub.service.ucloud.cn/YOUR_UHUB_REGISTRY/bert:chinese_base /bin/bash -c "cd /data&&python /data/run_classifier.py 
	--task_name=XNLI 
	--do_train=true 
	--do_eval=true 
	--data_dir=/data/data/ 
	--vocab_file=/data/chinese_L-12_H-768_A-12/vocab.txt 
	--bert_config_file=/data/chinese_L-12_H-768_A-12/bert_config.json 
	--init_checkpoint=/data/chinese_L-12_H-768_A-12/bert_model.ckpt 
	--max_seq_length=128 
	--train_batch_size=32 
	--learning_rate=5e-5 
	--num_train_epochs=2.0 
	--output_dir=/data/output/xnli_output/"
```

#### Running the Training on UAI Platform
We recommand you prepaing the training data using UFS (UCloud File System). UFS is a NFS liked system that you can mount it to your local dir (e.g., /ufs/) using nfs cmds. The you can use following CMD to copy data into UFS:

```
mkdir /ufs/bert/
cp -r /PATH_TO_XNLI-MT-1.0/multinli/ /ufs/bert/
cp /PATH_TO_XNLI-1.0/* /ufs/bert/
```

Then you can use the image you built to run the BERT training on UAI Train Platform with only a single CMD:

```
	/data/run_classifier.py 
		--task_name=XNLI 
		--do_train=true 
		--do_eval=true 
		--data_dir=/data/data/ 
		--vocab_file=/data/chinese_L-12_H-768_A-12/vocab.txt 
		--bert_config_file=/data/chinese_L-12_H-768_A-12/bert_config.json 
		--init_checkpoint=/data/chinese_L-12_H-768_A-12/bert_model.ckpt 
		--max_seq_length=128 
		--train_batch_size=32 
		--learning_rate=5e-5 
		--num_train_epochs=2.0 
		--output_dir=/data/output/xnli_output/
```

The platform will schedule your task to the GPU node and run the training task. For more details please see https://docs.ucloud.cn/ai/uai-train.