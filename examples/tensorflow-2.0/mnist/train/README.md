# Mnist Example for Tensorflow-2.0
This is an example of running mnist with Tensorflow-2.0. It includes a single GPU example and a distributed training example.

## Single GPU Example
This example shows how to run mnist on UAI Platform. The construction of the deep model is identical to https://www.tensorflow.org/tutorials. We add several modifications:

1. We use absl to handle input args. As Tensorflow-2.0 discard tf.app.flags, we use absl instead.
2. We add 4 input args (L35-38), these arguments are autogenerated by UAI Platform to infom the process the data location, the underlying GPU count, etc.

Our mnist example loads data from disk instead of downloading it from web. (Refer to L45 for more details). It also saves the output mode as .h5 file (Refer to L59 for more details)

### Build Training Docker Image
We provide mnist.Dockerfile for you to build the training docker image yourself. Take the following CMD to build your own image:

```
sudo docker build -t uhub.service.ucloud.cn/YOUR_UHUB_REGISTRY/mnist_single:v1.0 -f mnist.Dockerfile .
```

You can push this image into your own UCLOUD hub.

### Running the Training
You can run the training locally as
```
sudo nvidia-docker run -v /PATH_TO_MNIST_DATA/:/data/data/ -v /PATH_TO_OUTPUT/:/data/output uhub.service.ucloud.cn/YOUR_UHUB_REGISTRY/mnist_single:v1.0 /bin/bash -c "cd /data&&python mnist.py --data_dir=/data/data/ --output_dir=/data/output"
```

You can use the same image to run the training on UAI Train Platform. For more details please see https://docs.ucloud.cn/ai/uai-train.

## Distributed Training Example
This example shows how to run mnist on UAI Platform with Tensorflow distributed training. The mnist_dist.py is the main entrance for training and mnist_data.py is the data loader.

We also absl to handle input args. As Tensorflow-2.0 discard tf.app.flags, we use absl instead. We also add 4 input args (L27-30), these arguments are autogenerated by UAI Platform to infom the process the data location, the underlying GPU count, etc.

We use tf.estimator to run the distributed training and use tf.distribute to automatically apply distributed training strategy to each tensor and ops. The main entrance of training is the func train(). It will do the following things:

1.Update the TF_CONFIG. The TF_CONFIG generated by UAI Train Platform for 
distributed training looks like (which works with the tf.estimator of TF-1.x):

    TF_CONFIG = {
        "cluster":{
            "master":["ip0:2222"],
            "ps":["ip0:2223","ip1:2223"],
            "worker":["ip1:2222"]},
        "task":{"type":"master","index":0},
        "environment":"cloud"
    }

But this is not compatable with tf.distribute.experimental.MultiWorkerMirroredStrategy, which accept the config as:

    TF_CONFIG = {
        "cluster":{
            "chief":["ip0:2222"],
            "ps":["ip0:2223","ip1:2223"],
            "worker":["ip1:2222"]},
        "task":{"type":"chief","index":0},
        "environment":"cloud"
    }

So we should trickily change it (L89-L103).

2.Generate distribution strategy. If it is a parameter server, we use the tf.distribute.experimental.ParameterServerStrategy(). If it is a worker server, we use the tf.distribute.experimental.MultiWorkerMirroredStrategy(). (L112-115)

3.We use the distribution strategy to generate an estimator RunConfig. (L116)

4.Build the estimator from keras model and apply the dist-config. (L117)

5.Run the training.

### Build Training Docker Image
We provide mnist.Dockerfile for you to build the training docker image yourself. Take the following CMD to build your own image:

```
sudo docker build -t uhub.service.ucloud.cn/YOUR_UHUB_REGISTRY/mnist_dist:v1.0 -f mnist.Dockerfile .
```

You can push this image into your own UCLOUD hub.

### Running the Training
You can use the image you built to run a distributed training on UAI Train Platform with only a single CMD: 

	/data/mnist_multi.py

The platform will schedule your task to the GPU nodes and config the training cluster for you. For more details please see https://docs.ucloud.cn/ai/uai-train.

