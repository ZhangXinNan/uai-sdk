# Imagenet(Resnet) Example for Tensorflow-2.0
This is an example of running Resnet Training on Imagenet dataset with Tensorflow-2.0. It includes single GPU example, single node multi-GPU example and distributed training example.

## Setup
You should follow https://github.com/tensorflow/models/tree/master/official/resnet to download and generate tfrecords of imagenet dataset.

## Intro
We made several modifications to the code of examples/tensorflow/train/imagenet to make it compatable with Tensorflow-2.0:

1. We use absl to handle input args, as Tensorflow-2.0 discard tf.app.flags, we use absl instead.
2. We port most of tf-1.x APIs to tf-2.0 compatable APIs.

### Single Node Training Example
This example provides how to run Resnet training on UAI Platform within a single node. It use tf.distribute.MirroredStrategy() to support multi-gpu run. Please refer to imagenet\_main.py for more details.

### Distributed Training Example
This example provides how to run Resnet training on UAI Platform with Tensorflow distributed training. The auto-distributed code is included in imagenet\_main.py. 

We use tf.estimator to run the distributed training and use tf.distribute to automatically apply distributed training strategy to each tensor and ops. The main entrance of training is the func train(). It will do the following things:

1.Update the TF_CONFIG. The TF_CONFIG generated by UAI Train Platform for 
distributed training looks like (which works with the tf.estimator of TF-1.x):

    TF_CONFIG = {
        "cluster":{
            "master":["ip0:2222"],
            "ps":["ip0:2223","ip1:2223"],
            "worker":["ip1:2222"]},
        "task":{"type":"master","index":0},
        "environment":"cloud"
    }

But this is not compatable with tf.distribute.experimental.MultiWorkerMirroredStrategy, which accept the config as:

    TF_CONFIG = {
        "cluster":{
            "chief":["ip0:2222"],
            "ps":["ip0:2223","ip1:2223"],
            "worker":["ip1:2222"]},
        "task":{"type":"chief","index":0},
        "environment":"cloud"
    }

So we should trickily change it (L227-L242)

2.Generate distribution strategy. If it is a parameter server, we use the tf.distribute.experimental.ParameterServerStrategy(). If it is a worker server, we use the tf.distribute.experimental.MultiWorkerMirroredStrategy(). If it is not a distributed 
training task, we use tf.distribute.MirroredStrategy().

3.We use the distribution strategy to generate an estimator RunConfig.

### Build Training Docker Image
We provide imagenet_tf2.0.Dockerfile for you to build the training docker image yourself. Take the following CMD to build your own image:

```
sudo docker build -t uhub.service.ucloud.cn/YOUR_UHUB_REGISTRY/imagenet_tf:tf2.0 -f imagenet_tf2.0.Dockerfile .
```

You can push this image into your own UCLOUD hub.

### Running the Training locally
You can run the training locally as
```
sudo nvidia-docker run -v /PATH_TO_IMAGENET_DATA/:/data/data/ -v /PATH_TO_OUTPUT/:/data/output uhub.service.ucloud.cn/YOUR_UHUB_REGISTRY/imagenet_tf:tf2.0 /bin/bash -c "cd /data&&python imagenet_main.py --data_dir=/data/data/ --output_dir=/data/output"
```

### Running the Training on UAI Platform
You can use the image you built to run a distributed training on UAI Train Platform with only a single CMD: 

	/data/imagenet_main.py  --train-batch-size=256 --resnet_size=101

The platform will schedule your task to the GPU nodes and config the training cluster for you. For more details please see https://docs.ucloud.cn/ai/uai-train.